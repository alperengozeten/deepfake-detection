{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random\n",
    "from torchvision.transforms import Normalize\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Unnormalize:\n",
    "    \"\"\"Converts an image tensor that was previously Normalize'd\n",
    "    back to an image with pixels in the range [0, 1].\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
    "        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
    "        return torch.clamp(tensor*std + mean, 0., 1.)\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)\n",
    "unnormalize_transform = Unnormalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 64\n",
    "metadata=pd.read_csv(r\"D:\\dataset\\archive\\metadata.csv\")\n",
    "crops_dir=r\"D:\\dataset\\archive\\faces_224\"\n",
    "\n",
    "def random_hflip(img, p=0.5):\n",
    "    \"\"\"Random horizontal flip.\"\"\"\n",
    "    if random.random() < p:\n",
    "        return cv2.flip(img, 1)\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_label(filename, cls, crops_dir, image_size, augment):\n",
    "    \"\"\"Loads an image into a tensor. Also returns its label.\"\"\"\n",
    "    img = cv2.imread(os.path.join(crops_dir, filename))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if augment: \n",
    "        img = random_hflip(img)\n",
    "\n",
    "    img = cv2.resize(img, (image_size, image_size))\n",
    "\n",
    "    img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n",
    "    img = normalize_transform(img)\n",
    "\n",
    "    target = 1 if cls == \"FAKE\" else 0\n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Face crops dataset.\n",
    "\n",
    "    Arguments:\n",
    "        crops_dir: base folder for face crops\n",
    "        df: Pandas DataFrame with metadata\n",
    "        split: if \"train\", applies data augmentation\n",
    "        image_size: resizes the image to a square of this size\n",
    "        sample_size: evenly samples this many videos from the REAL\n",
    "            and FAKE subfolders (None = use all videos)\n",
    "        seed: optional random seed for sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, crops_dir, df, split, image_size, sample_size=None, seed=None):\n",
    "        self.crops_dir = crops_dir\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        if sample_size is not None:\n",
    "            real_df = df[df[\"label\"] == \"REAL\"]\n",
    "            fake_df = df[df[\"label\"] == \"FAKE\"]\n",
    "            sample_size = np.min(np.array([sample_size, len(real_df), len(fake_df)]))\n",
    "            print(\"%s: sampling %d from %d real videos\" % (split, sample_size, len(real_df)))\n",
    "            print(\"%s: sampling %d from %d fake videos\" % (split, sample_size, len(fake_df)))\n",
    "            real_df = real_df.sample(sample_size, random_state=seed)\n",
    "            fake_df = fake_df.sample(sample_size, random_state=seed)\n",
    "            self.df = pd.concat([real_df, fake_df])\n",
    "        else:\n",
    "            self.df = df\n",
    "\n",
    "        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n",
    "        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n",
    "        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        filename = row[\"videoname\"][:-4] + \".jpg\"\n",
    "        cls = row[\"label\"]\n",
    "        return load_image_and_label(filename, cls, self.crops_dir, \n",
    "                                    self.image_size, self.split == \"train\")\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_splits(crops_dir, metadata_df, frac):\n",
    "#     # Make a validation split. Sample a percentage of the real videos, \n",
    "#     # and also grab the corresponding fake videos.\n",
    "#     frac=frac*2\n",
    "#     real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n",
    "#     real_df = real_rows.sample(frac=frac, random_state=666)\n",
    "#     fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n",
    "#     r=len(real_df)\n",
    "#     f=len(fake_df)\n",
    "#     real_1=real_df[:int(r/2)]\n",
    "#     real_2=real_df[int(r/2):]\n",
    "#     fake_1=fake_df[:int(f/2)]\n",
    "#     fake_2=fake_df[int(f/2):]   \n",
    "#     val_df = pd.concat([real_1, fake_1])\n",
    "#     test_df=pd.concat([real_2, fake_2])\n",
    "\n",
    "#     #shuffle\n",
    "#     val_df=val_df.sample(frac=1,random_state=666)\n",
    "#     test_df=test_df.sample(frac=1,random_state=666)\n",
    "\n",
    "#     # The training split is the remaining videos.\n",
    "#     train_df = metadata_df.loc[~(metadata_df.index.isin( val_df.index) | metadata_df.index.isin( test_df.index))]\n",
    "\n",
    "#     return train_df, val_df, test_df\n",
    "def make_splits(crops_dir, metadata_df, frac):\n",
    "    # Make a validation split. Sample a percentage of the real videos, \n",
    "    # and also grab the corresponding fake videos.\n",
    "    real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n",
    "    real_df = real_rows.sample(frac=1, random_state=666)\n",
    "    fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n",
    "    fake_df=fake_df.sample(frac=1,random_state=666)[:len(real_df)]\n",
    "    r=len(real_df)\n",
    "    f=len(fake_df)\n",
    "\n",
    "    real_1=real_df[:int(8*r/10)]\n",
    "    real_2=real_df[int(8*r/10):int(9*r/10)]\n",
    "    real_3=real_df[int(9*r/10):]\n",
    "    fake_1=fake_df[:int(8*r/10)]\n",
    "    fake_2=fake_df[int(8*r/10):int(9*r/10)]  \n",
    "    fake_3=fake_df[int(9*r/10):] \n",
    "\n",
    "    val_df = pd.concat([real_2, fake_2])\n",
    "    test_df=pd.concat([real_3, fake_3])\n",
    "    train_df=pd.concat([real_1, fake_1])\n",
    "\n",
    "    train_df=train_df.sample(frac=1,random_state=666)\n",
    "    val_df=val_df.sample(frac=1,random_state=666)\n",
    "    test_df=test_df.sample(frac=1,random_state=666)\n",
    "    \n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(crops_dir, metadata_df, image_size, batch_size, num_workers):\n",
    "    train_df, val_df,test_df = make_splits(crops_dir, metadata_df, frac=0.05)\n",
    "\n",
    "    train_dataset = VideoDataset(crops_dir, train_df, \"train\", image_size, sample_size=None)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    val_dataset = VideoDataset(crops_dir, val_df, \"val\", image_size, sample_size=None, seed=1234)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    test_dataset = VideoDataset(crops_dir, test_df, \"test\", image_size, sample_size=None, seed=4321)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset has 13034 real videos, 13034 fake videos\n",
      "val dataset has 1629 real videos, 1629 fake videos\n",
      "test dataset has 1630 real videos, 1630 fake videos\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(crops_dir, metadata, image_size, \n",
    "                                               batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Propsoed CNN architecture.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Pamameters Initialization\n",
    "        input_shape = (3,224,224)\n",
    "        activation = nn.ReLU()\n",
    "        padding = 1\n",
    "        droprate = 0.1\n",
    "        epsilon=0.001\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=input_shape[0]),\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=16, eps=epsilon)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=64, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=128, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=256, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=512, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Propsoed CaN architecture.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "class CAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CAN, self).__init__()\n",
    "\n",
    "        # Pamameters Initialization\n",
    "        input_shape = (3,224,224)\n",
    "        activation = nn.Sigmoid()\n",
    "        padding = 1\n",
    "        droprate = 0.1\n",
    "        epsilon=0.001\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=input_shape[0]),\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=8, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=8, eps=epsilon)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=16, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=64, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=128, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=256, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=512, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size= 224 * 224, num_channels = 3, num_classes=2, fmap_base=16384, fmap_decay=1.0, fmap_max=512):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.num_channels = num_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.fmap_base = fmap_base\n",
    "        self.fmap_decay = fmap_decay\n",
    "        self.fmap_max = fmap_max\n",
    "\n",
    "        self.fromrgb = nn.Conv2d(num_channels, int(self.get_nf(0)), kernel_size=1, stride=1, padding=0)\n",
    "        self.main = nn.Sequential(\n",
    "            GBlock(self.get_nf(1), self.get_nf(1), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(1), self.get_nf(2), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(2), self.get_nf(3), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(3), self.get_nf(4), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(4), self.get_nf(5), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(5), self.get_nf(6), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(6), self.get_nf(7), num_classes=num_classes),\n",
    "            GBlock(self.get_nf(7), self.get_nf(8), num_classes=num_classes),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            self.fc = nn.Linear(self.get_nf(8), num_classes, bias=True)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.get_nf(8), 1, bias=True)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = F.leaky_relu(self.fromrgb(x), negative_slope=0.2)\n",
    "        h = self.main(h)\n",
    "        out = self.fc(h)\n",
    "        return out.squeeze()\n",
    "\n",
    "    def get_nf(self, stage):\n",
    "        return min(int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), self.fmap_max)\n",
    "\n",
    "class GBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, num_classes=0):\n",
    "        super(GBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.act1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.act2 = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.norm1 = PixelNorm()\n",
    "        self.norm2 = PixelNorm()\n",
    "        self.skip_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "        self.skip_norm = PixelNorm()\n",
    "        self.num_classes = num_classes\n",
    "        if self.num_classes > 0:\n",
    "            self.cbn1 = nn.BatchNorm2d(in_channels, num_classes)\n",
    "            self.cbn2 = nn.BatchNorm2d(out_channels, num_classes)\n",
    "        else:\n",
    "            self.cbn1 = nn.BatchNorm2d(in_channels)\n",
    "            self.cbn2 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        h = self\n",
    "\n",
    "model = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "[1,   100] loss: 0.689\n",
      "[1,   200] loss: 0.677\n",
      "[1,   300] loss: 0.664\n",
      "[1,   400] loss: 0.668\n",
      "[1] validation loss: 0.666\n"
     ]
    }
   ],
   "source": [
    "#Training phase imp:\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "best_val_loss = float('inf') # initialize with a very high value\n",
    "\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch\", epoch)\n",
    "    for index, data in enumerate(train_loader):\n",
    "        # print(\"input is \", data[0], \"and label is \", data[1])\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if index % 100 == 99:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, index + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    # Validate the model on validation dataset\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print('[%d] validation loss: %.3f' % (epoch + 1, val_loss))\n",
    "\n",
    "    # Save the model with the best validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation images: 61 %\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, test_loader, checkpoint_path='best_model.pth'):\n",
    "    # load the best model checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    " \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #TODO: For calculation of precision, accuracy, recall and F1 score, get related elements.\n",
    "\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "test_model(model, test_loader, 'best_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs464hws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
