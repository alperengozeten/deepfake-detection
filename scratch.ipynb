{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random\n",
    "from torchvision.transforms import Normalize\n",
    "import cv2\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Unnormalize:\n",
    "    \"\"\"Converts an image tensor that was previously Normalize'd\n",
    "    back to an image with pixels in the range [0, 1].\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
    "        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
    "        return torch.clamp(tensor*std + mean, 0., 1.)\n",
    "\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "normalize_transform = Normalize(mean, std)\n",
    "unnormalize_transform = Unnormalize(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 160\n",
    "batch_size = 64\n",
    "metadata=pd.read_csv(r\"C:\\\\Users\\\\PC\\Desktop\\\\CS464\\\\face-verification\\\\archive\\\\metadata.csv\")\n",
    "crops_dir=r\"C:\\\\Users\\\\PC\\Desktop\\\\CS464\\\\face-verification\\\\archive\\\\faces_224\"\n",
    "\n",
    "# def random_hflip(img, p=0.5):\n",
    "#     \"\"\"Random horizontal flip.\"\"\"\n",
    "#     if random.random() < p:\n",
    "#         return cv2.flip(img, 1)\n",
    "#     else:\n",
    "#         return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_hflip(img, p=1):\n",
    "    \"\"\"Random horizontal flip.\"\"\"\n",
    "    if random.random() < p:\n",
    "        return cv2.flip(img, 1)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "def rotate(image, angle=90, scale=1.0):\n",
    "    '''\n",
    "    Rotate the image\n",
    "    :param image: image to be processed\n",
    "    :param angle: Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).\n",
    "    :param scale: Isotropic scale factor.\n",
    "    '''\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    #rotate matrix\n",
    "    M = cv2.getRotationMatrix2D((w/2,h/2), angle, scale)\n",
    "    #rotate\n",
    "    image = cv2.warpAffine(image,M,(w,h))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image_and_label(filename, cls, crops_dir, image_size, augment):\n",
    "#     \"\"\"Loads an image into a tensor. Also returns its label.\"\"\"\n",
    "#     img = cv2.imread(os.path.join(crops_dir, filename))\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     if augment: \n",
    "#         img = random_hflip(img)\n",
    "\n",
    "#     img = cv2.resize(img, (image_size, image_size))\n",
    "\n",
    "#     img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n",
    "#     img = normalize_transform(img)\n",
    "\n",
    "#     target = 1 if cls == \"FAKE\" else 0\n",
    "#     return img, target\n",
    "def load_image_and_label(filename, cls, crops_dir, image_size, augment=0):\n",
    "    \"\"\"Loads an image into a tensor. Also returns its label.\"\"\"\n",
    "    img = cv2.imread(os.path.join(crops_dir, filename))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if cls== \"REAL\":\n",
    "        if augment==1: \n",
    "        \n",
    "            img = cv2.flip(img,1)\n",
    "        \n",
    "        elif augment==2:\n",
    "        \n",
    "            img=cv2.flip(img,0)\n",
    "        \n",
    "        elif augment==3:\n",
    "        \n",
    "            img=rotate(img,90,1.0)    \n",
    "        \n",
    "        elif augment==4:\n",
    "        \n",
    "            img=rotate(img,270,1.0)   \n",
    "\n",
    "    img = cv2.resize(img, (image_size, image_size))\n",
    "\n",
    "    img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n",
    "    img = normalize_transform(img)\n",
    "\n",
    "    target = 1 if cls == \"FAKE\" else 0\n",
    "    return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Face crops dataset.\n",
    "\n",
    "    Arguments:\n",
    "        crops_dir: base folder for face crops\n",
    "        df: Pandas DataFrame with metadata\n",
    "        split: if \"train\", applies data augmentation\n",
    "        image_size: resizes the image to a square of this size\n",
    "        sample_size: evenly samples this many videos from the REAL\n",
    "            and FAKE subfolders (None = use all videos)\n",
    "        seed: optional random seed for sampling\n",
    "    \"\"\"\n",
    "    def __init__(self, crops_dir, df, split, image_size,augment=False, oversample=False, sample_size=None, seed=None):\n",
    "        self.crops_dir = crops_dir\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        self.augment=augment\n",
    "        self.oversample = oversample\n",
    "\n",
    "        if sample_size is not None:\n",
    "            real_df = df[df[\"label\"] == \"REAL\"]\n",
    "            fake_df = df[df[\"label\"] == \"FAKE\"]\n",
    "            sample_size = np.min(np.array([sample_size, len(real_df), len(fake_df)]))\n",
    "            print(\"%s: sampling %d from %d real videos\" % (split, sample_size, len(real_df)))\n",
    "            print(\"%s: sampling %d from %d fake videos\" % (split, sample_size, len(fake_df)))\n",
    "            real_df = real_df.sample(sample_size, random_state=seed)\n",
    "            fake_df = fake_df.sample(sample_size, random_state=seed)\n",
    "            self.df = pd.concat([real_df, fake_df])\n",
    "            self.df['aug'] = np.zeros((len(self.df), 1))\n",
    "        elif self.augment:\n",
    "            real_df = df[df[\"label\"] == \"REAL\"]\n",
    "            real_df[\"aug\"]=np.zeros((len(real_df),1))\n",
    "            \n",
    "            real_df1=real_df.copy()\n",
    "            real_df1[\"aug\"]=np.ones((len(real_df),1))\n",
    "            \n",
    "            real_df2=real_df.copy()\n",
    "            real_df2[\"aug\"]=np.ones((len(real_df),1))*2\n",
    "            \n",
    "            real_df3=real_df.copy()\n",
    "            real_df3[\"aug\"]=np.ones((len(real_df),1))*3\n",
    "            \n",
    "            real_df4=real_df.copy()\n",
    "            real_df4[\"aug\"]=np.ones((len(real_df),1))*4\n",
    "            \n",
    "            fake_df = df[df[\"label\"] == \"FAKE\"]\n",
    "            fake_df[\"aug\"]=np.zeros((len(fake_df),1))\n",
    "            \n",
    "            self.df = pd.concat([real_df, real_df1, real_df2, real_df3, real_df4, fake_df])\n",
    "        elif self.oversample: # oversample the real data\n",
    "            real_df = df[df[\"label\"] == \"REAL\"] \n",
    "            fake_df = df[df[\"label\"] == \"FAKE\"]\n",
    "            \n",
    "            real_df1 = real_df.copy()\n",
    "            real_df2 = real_df.copy()\n",
    "            real_df3 = real_df.copy()\n",
    "            real_df4 = real_df.copy()\n",
    "            self.df = pd.concat([real_df, real_df1, real_df2, real_df3, real_df4, fake_df])\n",
    "            self.df['aug'] = np.zeros((len(self.df), 1))\n",
    "        else:\n",
    "            self.df = df\n",
    "\n",
    "        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n",
    "        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n",
    "        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        filename = row[\"videoname\"][:-4] + \".jpg\"\n",
    "        cls = row[\"label\"]\n",
    "        return load_image_and_label(filename, cls, self.crops_dir, \n",
    "                                    self.image_size, self.split == \"train\")\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_splits(crops_dir, metadata_df, frac):\n",
    "    # Make a validation split. Sample a percentage of the real videos, \n",
    "    # and also grab the corresponding fake videos.\n",
    "    frac=frac*2\n",
    "    real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n",
    "    real_df = real_rows.sample(frac=frac, random_state=666)\n",
    "    fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n",
    "    r=len(real_df)\n",
    "    f=len(fake_df)\n",
    "    real_1=real_df[:int(r/2)]\n",
    "    real_2=real_df[int(r/2):]\n",
    "    fake_1=fake_df[:int(f/2)]\n",
    "    fake_2=fake_df[int(f/2):]   \n",
    "    val_df = pd.concat([real_1, fake_1])\n",
    "    test_df=pd.concat([real_2, fake_2])\n",
    "\n",
    "    #shuffle\n",
    "    val_df=val_df.sample(frac=1,random_state=666)\n",
    "    test_df=test_df.sample(frac=1,random_state=666)\n",
    "\n",
    "    # The training split is the remaining videos.\n",
    "    train_df = metadata_df.loc[~(metadata_df.index.isin( val_df.index) | metadata_df.index.isin( test_df.index))]\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "# def make_splits(crops_dir, metadata_df, frac):\n",
    "#     # Make a validation split. Sample a percentage of the real videos, \n",
    "#     # and also grab the corresponding fake videos.\n",
    "#     real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n",
    "#     real_df = real_rows.sample(frac=1, random_state=666)\n",
    "#     fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n",
    "#     fake_df=fake_df.sample(frac=1,random_state=666)[:len(real_df)]\n",
    "#     r=len(real_df)\n",
    "#     f=len(fake_df)\n",
    "\n",
    "#     real_1=real_df[:int(8*r/10)]\n",
    "#     real_2=real_df[int(8*r/10):int(9*r/10)]\n",
    "#     real_3=real_df[int(9*r/10):]\n",
    "#     fake_1=fake_df[:int(8*r/10)]\n",
    "#     fake_2=fake_df[int(8*r/10):int(9*r/10)]  \n",
    "#     fake_3=fake_df[int(9*r/10):] \n",
    "\n",
    "#     val_df = pd.concat([real_2, fake_2])\n",
    "#     test_df=pd.concat([real_3, fake_3])\n",
    "#     train_df=pd.concat([real_1, fake_1])\n",
    "\n",
    "#     train_df=train_df.sample(frac=1,random_state=666)\n",
    "#     val_df=val_df.sample(frac=1,random_state=666)\n",
    "#     test_df=test_df.sample(frac=1,random_state=666)\n",
    "    \n",
    "#     return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(crops_dir, metadata_df, image_size, batch_size, num_workers=0):\n",
    "    train_df, val_df,test_df = make_splits(crops_dir, metadata_df, frac=0.05)\n",
    "\n",
    "    train_dataset = VideoDataset(crops_dir, train_df, \"train\", image_size,augment=False, oversample=True, sample_size=None)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    val_dataset = VideoDataset(crops_dir, val_df, \"val\", image_size,augment=False, oversample=True, sample_size=None, seed=1234)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    \n",
    "    test_dataset = VideoDataset(crops_dir, test_df, \"test\", image_size, sample_size=None, seed=4321)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, \n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(crops_dir, metadata, image_size, \n",
    "                                               batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": val_loader,\n",
    "    \"test\": test_loader\n",
    "}\n",
    "dataset_sizes = {\n",
    "    \"train\": len(train_loader.dataset),\n",
    "    \"val\": len(val_loader.dataset),\n",
    "    \"test\": len(test_loader.dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "\n",
    "#         # Pamameters Initialization\n",
    "#         input_shape = (3,224,224)\n",
    "#         activation = nn.ReLU()\n",
    "#         padding = 1\n",
    "#         droprate = 0.1\n",
    "#         epsilon=0.001\n",
    "\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.BatchNorm2d(num_features=input_shape[0]),\n",
    "#             nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=16, eps=epsilon)\n",
    "#         )\n",
    "\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=32, eps=epsilon),\n",
    "#             nn.Dropout2d(p=droprate)\n",
    "#         )\n",
    "\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=64, eps=epsilon),\n",
    "#             nn.Dropout2d(p=droprate)\n",
    "#         )\n",
    "\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=128, eps=epsilon),\n",
    "#             nn.Dropout2d(p=droprate)\n",
    "#         )\n",
    "\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=256, eps=epsilon),\n",
    "#             nn.Dropout2d(p=droprate)\n",
    "#         )\n",
    "\n",
    "#         self.layer6 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=padding),\n",
    "#             activation,\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.BatchNorm2d(num_features=512, eps=epsilon),\n",
    "#             nn.Dropout2d(p=droprate)\n",
    "#         )\n",
    "\n",
    "#         self.layer7 = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(512, 2),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = self.layer6(x)\n",
    "#         x = self.layer7(x)\n",
    "#         return x\n",
    "\n",
    "# model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Propsoed CNN architecture.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Pamameters Initialization\n",
    "        input_shape = (3,224,224)\n",
    "        activation = nn.Sigmoid()\n",
    "        padding = 1\n",
    "        droprate = 0.1\n",
    "        epsilon=0.001\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(num_features=input_shape[0]),\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=8, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=8, eps=epsilon)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=16, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=32, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=64, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=128, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=256, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=padding),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(num_features=512, eps=epsilon),\n",
    "            nn.Dropout2d(p=droprate)\n",
    "        )\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class PixelNorm(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(PixelNorm, self).__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, img_size= 224 * 224, num_channels = 3, num_classes=2, fmap_base=16384, fmap_decay=1.0, fmap_max=512):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.img_size = img_size\n",
    "#         self.num_channels = num_channels\n",
    "#         self.num_classes = num_classes\n",
    "#         self.fmap_base = fmap_base\n",
    "#         self.fmap_decay = fmap_decay\n",
    "#         self.fmap_max = fmap_max\n",
    "\n",
    "#         self.fromrgb = nn.Conv2d(num_channels, int(self.get_nf(0)), kernel_size=1, stride=1, padding=0)\n",
    "#         self.main = nn.Sequential(\n",
    "#             GBlock(self.get_nf(1), self.get_nf(1), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(1), self.get_nf(2), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(2), self.get_nf(3), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(3), self.get_nf(4), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(4), self.get_nf(5), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(5), self.get_nf(6), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(6), self.get_nf(7), num_classes=num_classes),\n",
    "#             GBlock(self.get_nf(7), self.get_nf(8), num_classes=num_classes),\n",
    "#             nn.LeakyReLU(negative_slope=0.2),\n",
    "#             nn.AdaptiveAvgPool2d((1, 1)),\n",
    "#             nn.Flatten(start_dim=1)\n",
    "#         )\n",
    "\n",
    "#         if self.num_classes > 0:\n",
    "#             self.fc = nn.Linear(self.get_nf(8), num_classes, bias=True)\n",
    "#         else:\n",
    "#             self.fc = nn.Linear(self.get_nf(8), 1, bias=True)\n",
    "\n",
    "#     def forward(self, x, y=None):\n",
    "#         h = F.leaky_relu(self.fromrgb(x), negative_slope=0.2)\n",
    "#         h = self.main(h)\n",
    "#         out = self.fc(h)\n",
    "#         return out.squeeze()\n",
    "\n",
    "#     def get_nf(self, stage):\n",
    "#         return min(int(self.fmap_base / (2.0 ** (stage * self.fmap_decay))), self.fmap_max)\n",
    "\n",
    "# class GBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, num_classes=0):\n",
    "#         super(GBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=padding)\n",
    "#         self.act1 = nn.LeakyReLU(negative_slope=0.2)\n",
    "#         self.act2 = nn.LeakyReLU(negative_slope=0.2)\n",
    "#         self.norm1 = PixelNorm()\n",
    "#         self.norm2 = PixelNorm()\n",
    "#         self.skip_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "#         self.skip_norm = PixelNorm()\n",
    "#         self.num_classes = num_classes\n",
    "#         if self.num_classes > 0:\n",
    "#             self.cbn1 = nn.BatchNorm2d(in_channels, num_classes)\n",
    "#             self.cbn2 = nn.BatchNorm2d(out_channels, num_classes)\n",
    "#         else:\n",
    "#             self.cbn1 = nn.BatchNorm2d(in_channels)\n",
    "#             self.cbn2 = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "#     def forward(self, x, y=None):\n",
    "#         h = self\n",
    "\n",
    "# model = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropy()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr scheduler\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 10):\n",
    "    loss_train=[]\n",
    "    acc_train=[]\n",
    "    loss_val=[]\n",
    "    acc_val=[]\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print(\"-\"*10)\n",
    "        \n",
    "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
    "            if phase == 'train':\n",
    "                model.train() # model to training mode\n",
    "            else:\n",
    "                model.eval() # model to evaluate\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step() # step at end of epoch\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
    "            if phase == 'train':\n",
    "                loss_train.append(epoch_loss)\n",
    "                acc_train.append(epoch_acc)\n",
    "            else:\n",
    "                loss_val.append(epoch_loss)\n",
    "                acc_val.append(epoch_acc)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
    "            loss_train.append(epoch_loss)\n",
    "            acc_train.append(epoch_acc)\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
    "        print()\n",
    "    time_elapsed = time.time() - since # slight error\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_train, acc_train, loss_val, acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, lt,at,lv,av = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10) # now it is a lot faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training phase imp:\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "best_val_loss = float('inf') # initialize with a very high value\n",
    "\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    print(\"epoch\", epoch)\n",
    "    for index, data in enumerate(train_loader):\n",
    "        # print(\"input is \", data[0], \"and label is \", data[1])\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if index % 100 == 99:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, index + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    # Validate the model on validation dataset\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for index, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "    print('[%d] validation loss: %.3f' % (epoch + 1, val_loss))\n",
    "\n",
    "    # Save the model with the best validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, checkpoint_path='best_model.pth'):\n",
    "    # load the best model checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    " \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0 \n",
    "        tn = 0\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #TODO: For calculation of precision, accuracy, recall and F1 score, get related elements.\n",
    "            \n",
    "            for gt_label, pred_label in zip(labels, predicted):\n",
    "                if (gt_label == 1 and pred_label == 1):\n",
    "                    tp = tp + 1\n",
    "                if (gt_label == 0 and pred_label == 1):\n",
    "                    fp = fp + 1\n",
    "                if (gt_label == 1 and pred_label == 0):\n",
    "                    fn = fn + 1\n",
    "                if (gt_label == 0 and pred_label == 0):\n",
    "                    tn = tn + 1\n",
    "    print(fp, fn, tp, tn)\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    return correct, fp, fn, tp, tn\n",
    "correct, fp, fn, tp, tn = test_model(model, test_loader, 'best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def showcm(fp,fn,tp,tn):\n",
    "\n",
    "        labels = ['+', '-']\n",
    "        \n",
    "        # Create the matrix\n",
    "        confusion_matrix = np.array([[tp, fp],\n",
    "                                    [fn, tn]])\n",
    "\n",
    "        # Plot the matrix\n",
    "        sns.set(font_scale=1.4)\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "        # Set plot properties\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "showcm(fp, fn, tp, tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the true positive, true negative, false positive, and false negative values\n",
    "print(\"True Positive (TP): \", tp)\n",
    "print(\"True Negative (TN): \", tn)\n",
    "print(\"False Positive (FP): \", fp)\n",
    "print(\"False Negative (FN): \", fn)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    " \n",
    "# Calculate precision\n",
    "precision = tp / (tp + fp)\n",
    " \n",
    "# Calculate recall\n",
    "recall = tp / (tp + fn)\n",
    " \n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Print the accuracy, precision, recall, and F1-score\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(\"Accuracy: \", round(accuracy, 2))\n",
    "print(\"Precision: \", round(precision, 2))\n",
    "print(\"Recall: \", round(recall, 2))\n",
    "print(\"F1-score: \", round(f1_score, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, [3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt2=[lt[a] for a in range(len(lt)) if a % 3 == 1 ]\n",
    "def plot_loss(epochs, loss, val_loss):\n",
    "    plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'orange', label = 'Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy(epochs, acc, val_acc):\n",
    "    plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'orange', label = 'Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_accuracy(range(1, len(av3) + 1), at3, av3)\n",
    "plot_loss(range(1, len(lt2) + 1), lt2, lv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs464hws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
