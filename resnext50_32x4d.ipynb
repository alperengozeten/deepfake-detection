{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "TZ_jgH_mIdCR",
        "outputId": "9304d6fa-3585-4aaf-a74f-0bbd2b1ef2f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7956ccc7-07a6-42e4-8311-1565d44b1494\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7956ccc7-07a6-42e4-8311-1565d44b1494\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"ouzcanduran\",\"key\":\"a9ec970310996c021e4930a0d8695ffd\"}'}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIC_e8ZlIir5",
        "outputId": "0bb87d71-0cf8-4901-dfb8-3786b2f3dbf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrsP0fvkJF0K",
        "outputId": "78a38cf0-64d8-4ec0-e65b-76394965f0cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading deepfake-faces.zip to /content\n",
            "100% 432M/433M [00:21<00:00, 22.4MB/s]\n",
            "100% 433M/433M [00:21<00:00, 21.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d dagnelies/deepfake-faces\n",
        "! mkdir 2043"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJyml1lOJRXd",
        "outputId": "a707a80d-804e-45aa-f643-ba06e8cdb730"
      },
      "outputs": [],
      "source": [
        "! unzip /content/deepfake-faces.zip -d 2043"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-0x2qJbJVkT",
        "outputId": "9f1e44e6-917c-44fc-d3a4-c2055351d901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.9/dist-packages (0.6.13)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/dist-packages (from timm) (0.13.4)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.15.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from torchvision.transforms import Normalize\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "## Now, we import timm, torchvision image models\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms as T # for simplifying the transforms\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, sampler, random_split\n",
        "from torchvision import models\n",
        "!pip install timm # kaggle doesnt have it installed by default\n",
        "import timm\n",
        "from timm.loss import LabelSmoothingCrossEntropy # This is better than normal nn.CrossEntropyLoss\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WWsa1TvqJeHb"
      },
      "outputs": [],
      "source": [
        "image_size = 224\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "XAWufRsZJjEu"
      },
      "outputs": [],
      "source": [
        "metadata=pd.read_csv(\"/content/2043/metadata.csv\")\n",
        "crops_dir=\"/content/2043/faces_224\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "w72WGxC_JlCI"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Unnormalize:\n",
        "    \"\"\"Converts an image tensor that was previously Normalize'd\n",
        "    back to an image with pixels in the range [0, 1].\"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
        "        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n",
        "        return torch.clamp(tensor*std + mean, 0., 1.)\n",
        "\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)\n",
        "unnormalize_transform = Unnormalize(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rEu2qpAMJu5V"
      },
      "outputs": [],
      "source": [
        "def random_hflip(img, p=0.5):\n",
        "    \"\"\"Random horizontal flip.\"\"\"\n",
        "    if random.random() < p:\n",
        "        return cv2.flip(img, 1)\n",
        "    else:\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vRbe6WXvJxGc"
      },
      "outputs": [],
      "source": [
        "def load_image_and_label(filename, cls, crops_dir, image_size, augment):\n",
        "    \"\"\"Loads an image into a tensor. Also returns its label.\"\"\"\n",
        "    img = cv2.imread(os.path.join(crops_dir, filename))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    if augment: \n",
        "        img = random_hflip(img)\n",
        "\n",
        "    img = cv2.resize(img, (image_size, image_size))\n",
        "\n",
        "    img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n",
        "    img = normalize_transform(img)\n",
        "\n",
        "    target = 1 if cls == \"FAKE\" else 0\n",
        "    return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "XaArqpRdJyW7"
      },
      "outputs": [],
      "source": [
        "class VideoDataset(Dataset):\n",
        "    \"\"\"Face crops dataset.\n",
        "\n",
        "    Arguments:\n",
        "        crops_dir: base folder for face crops\n",
        "        df: Pandas DataFrame with metadata\n",
        "        split: if \"train\", applies data augmentation\n",
        "        image_size: resizes the image to a square of this size\n",
        "        sample_size: evenly samples this many videos from the REAL\n",
        "            and FAKE subfolders (None = use all videos)\n",
        "        seed: optional random seed for sampling\n",
        "    \"\"\"\n",
        "    def __init__(self, crops_dir, df, split, image_size, sample_size=None, seed=None):\n",
        "        self.crops_dir = crops_dir\n",
        "        self.split = split\n",
        "        self.image_size = image_size\n",
        "        \n",
        "        if sample_size is not None:\n",
        "            real_df = df[df[\"label\"] == \"REAL\"]\n",
        "            fake_df = df[df[\"label\"] == \"FAKE\"]\n",
        "            sample_size = np.min(np.array([sample_size, len(real_df), len(fake_df)]))\n",
        "            print(\"%s: sampling %d from %d real videos\" % (split, sample_size, len(real_df)))\n",
        "            print(\"%s: sampling %d from %d fake videos\" % (split, sample_size, len(fake_df)))\n",
        "            real_df = real_df.sample(sample_size, random_state=seed)\n",
        "            fake_df = fake_df.sample(sample_size, random_state=seed)\n",
        "            self.df = pd.concat([real_df, fake_df])\n",
        "        else:\n",
        "            self.df = df\n",
        "\n",
        "        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n",
        "        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n",
        "        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        filename = row[\"videoname\"][:-4] + \".jpg\"\n",
        "        cls = row[\"label\"]\n",
        "        return load_image_and_label(filename, cls, self.crops_dir, \n",
        "                                    self.image_size, self.split == \"train\")\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L7FdBTZ5scNh",
        "outputId": "cdaa8b8f-0ca2-4dea-9536-a65d69b20bcd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/2043/faces_224'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crops_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "wjPa30mcJ9DX"
      },
      "outputs": [],
      "source": [
        "def make_splits(crops_dir, metadata_df, frac):\n",
        "    # Make a validation split. Sample a percentage of the real videos, \n",
        "    # and also grab the corresponding fake videos.\n",
        "    frac=frac*2\n",
        "    real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n",
        "    real_df = real_rows.sample(frac=frac, random_state=666)\n",
        "    fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n",
        "    r=len(real_df)\n",
        "    f=len(fake_df)\n",
        "    real_1=real_df[:int(r/2)]\n",
        "    real_2=real_df[int(r/2):]\n",
        "    fake_1=fake_df[:int(f/2)]\n",
        "    fake_2=fake_df[int(f/2):]   \n",
        "    val_df = pd.concat([real_1, fake_1])\n",
        "    test_df=pd.concat([real_2, fake_2])\n",
        "\n",
        "    #shuffle\n",
        "    val_df=val_df.sample(frac=1,random_state=666)\n",
        "    test_df=test_df.sample(frac=1,random_state=666)\n",
        "\n",
        "    # The training split is the remaining videos.\n",
        "    train_df = metadata_df.loc[~(metadata_df.index.isin( val_df.index) | metadata_df.index.isin( test_df.index))]\n",
        "\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "W0XFqo9rJ_zL"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(crops_dir, metadata_df, image_size, batch_size, num_workers):\n",
        "    train_df, val_df,test_df = make_splits(crops_dir, metadata_df, frac=0.05)\n",
        "\n",
        "    train_dataset = VideoDataset(crops_dir, train_df, \"train\", image_size, sample_size=None)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    val_dataset = VideoDataset(crops_dir, val_df, \"val\", image_size, sample_size=None, seed=1234)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, \n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "    \n",
        "    test_dataset = VideoDataset(crops_dir, test_df, \"test\", image_size, sample_size=None, seed=4321)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, \n",
        "                            num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLye8-KZKFgj",
        "outputId": "d9aab36a-cf99-444a-ca0b-ecf6f9da9829"
      },
      "outputs": [],
      "source": [
        "train_loader, val_loader, test_loader = create_data_loaders(crops_dir, metadata, image_size, \n",
        "                                               batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "DoY5Kd4CKK1e"
      },
      "outputs": [],
      "source": [
        "dataloaders = {\n",
        "    \"train\": train_loader,\n",
        "    \"val\": val_loader,\n",
        "    \"test\": test_loader\n",
        "}\n",
        "dataset_sizes = {\n",
        "    \"train\": len(train_loader.dataset),\n",
        "    \"val\": len(val_loader.dataset),\n",
        "    \"test\": len(test_loader.dataset)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSgJXBnnKlAQ",
        "outputId": "6420f56c-d718-42c3-a4bc-e7376042c4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAu0arxHqwT4",
        "outputId": "e72dfad1-eccd-4497-984d-e9796a9c79bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        }
      ],
      "source": [
        "#model= torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b4', pretrained=True)\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Zks6MYmSyEJB"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters(): #freeze model\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model.fc.in_features \n",
        "model.fc = nn.Linear(n_inputs, 2)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VajSQOqCGc2j",
        "outputId": "74488194-dc0d-48da-e706-43c0e28908d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=2048, out_features=2, bias=True)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "KYYg1r1X0Rb7"
      },
      "outputs": [],
      "source": [
        "\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "criterion = criterion.to(device)\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.01 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "ZLIKB7V1KtuB"
      },
      "outputs": [],
      "source": [
        "# lr scheduler\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "qVF3UCfvKvHG"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs = 20):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print(\"-\"*10)\n",
        "        \n",
        "        for phase in ['train', 'val']: # We do training and validation phase per epoch\n",
        "            if phase == 'train':\n",
        "                model.train() # model to training mode\n",
        "            else:\n",
        "                model.eval() # model to evaluate\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "            \n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'): # no autograd makes validation go faster\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # used for accuracy\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step() # step at end of epoch\n",
        "            \n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc =  running_corrects.double() / dataset_sizes[phase]\n",
        "            \n",
        "            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n",
        "            \n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict()) # keep the best validation accuracy model\n",
        "        print()\n",
        "    time_elapsed = time.time() - since # slight error\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n",
        "    \n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaoWKcgwKw5v",
        "outputId": "d21b7723-3309-4311-99c4-7b93fd9b516c"
      },
      "outputs": [],
      "source": [
        "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "tawCz4L-uOH7"
      },
      "outputs": [],
      "source": [
        "classes=['training_real', 'training_fake']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO6y9vBFKzwq",
        "outputId": "db18d511-10a6-4601-bbec-b04251d2e01f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 74/74 [00:21<00:00,  3.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.0034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0.0\n",
        "class_correct = list(0 for i in range(2))\n",
        "class_total = list(0 for i in range(2))\n",
        "model.eval()\n",
        "\n",
        "for data, target in tqdm(test_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "   \n",
        "    if torch.cuda.is_available():\n",
        "      #model.cuda()\n",
        "      model= model.to(device)\n",
        "    with torch.no_grad(): # turn off autograd for faster testing\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "    test_loss = loss.item() * data.size(0)\n",
        "    _, pred = torch.max(output, 1)\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    if len(target) == 64:\n",
        "        for i in range(64):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "test_loss = test_loss / len(test_loader.dataset) \n",
        "print('Test Loss: {:.4f}'.format(test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGeRj-6ltxJ6",
        "outputId": "b18757c5-cd5d-45ed-a8de-9fcd508cc695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy of training_real: 19% (155/806)\n",
            "Test Accuracy of training_fake: 96% (3731/3866)\n",
            "Test Accuracy of 83% (3886/4672)\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(classes)):\n",
        "    if class_total[i] > 0:\n",
        "        print(\"Test Accuracy of %5s: %2d%% (%2d/%2d)\" % (\n",
        "            classes[i], 100*class_correct[i]/class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])\n",
        "        ))\n",
        "    else:\n",
        "        print(\"Test accuracy of %5s: NA\" % (classes[i]))\n",
        "print(\"Test Accuracy of %2d%% (%2d/%2d)\" % (\n",
        "            100*np.sum(class_correct)/np.sum(class_total), np.sum(class_correct), np.sum(class_total)\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "E7igcRfPv5PS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
